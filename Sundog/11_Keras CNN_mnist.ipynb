{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras - MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Conv2D, MaxPooling2D, Flatten\n",
    "from keras.optimizers import RMSprop\n",
    "from keras import backend as K\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "(mnist_train_images, mnist_train_labels), (mnist_test_images, mnist_test_labels) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist_train_images.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to shape the data differently then before. Since we're treating the data as 2D images of 28x28 pixels instead of a flattened stream of 784 pixels, we need to shape it accordingly. Depending on the data format Keras is set up for, this may be 1x28x28 or 28x28x1 (the \"1\" indicates a single color channel, as this is just grayscale. If we were dealing with color images, it would be 3 instead of 1 since we'd have red, green, and blue color channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'channels_last'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "K.image_data_format()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "if K.image_data_format() == 'channels_first':\n",
    "    train_images = mnist_train_images.reshape(mnist_train_images.shape[0], 1, 28, 28)\n",
    "    test_images = mnist_test_images.reshape(mnist_test_images.shape[0], 1, 28, 28)\n",
    "    input_shape = (1, 28, 28)\n",
    "else:\n",
    "    train_images = mnist_train_images.reshape(mnist_train_images.shape[0], 28, 28, 1)\n",
    "    test_images = mnist_test_images.reshape(mnist_test_images.shape[0], 28, 28, 1)\n",
    "    input_shape = (28, 28, 1)\n",
    "    \n",
    "train_images = train_images.astype('float32')\n",
    "test_images = test_images.astype('float32')\n",
    "train_images /= 255\n",
    "test_images /= 255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert our train and test labels to be categorical in one-hot format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = keras.utils.to_categorical(mnist_train_labels, 10)\n",
    "test_labels = keras.utils.to_categorical(mnist_test_labels, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a sanity check let's print out one of the training images with its label:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_sample(num):\n",
    "    # Print the one-hot array of this sample's label \n",
    "    print(train_labels[num])  \n",
    "    \n",
    "    # Print the label converted back to a number\n",
    "    label = train_labels[num].argmax(axis=0)\n",
    "    \n",
    "    # Reshape the 768 values to a 28x28 image\n",
    "    image = train_images[num].reshape([28,28])\n",
    "    \n",
    "    plt.title('Sample: %d  Label: %d' % (num, label))\n",
    "    plt.imshow(image, cmap=plt.get_cmap('gray_r'))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1dbe1428438>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_sample(np.random.randint(60000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=input_shape))\n",
    "\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))  # 64 3x3 kernels\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))         # Reduce by taking the max of each 2x2 block\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "# Flatten the results to one dimension for passing into our final layer\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "# Final categorization from 0-9 with softmax\n",
    "model.add(Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_3 (Conv2D)            (None, 26, 26, 32)        320       \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 24, 24, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 12, 12, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 12, 12, 64)        0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 9216)              0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 128)               1179776   \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 1,199,882\n",
      "Trainable params: 1,199,882\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(train_images, train_labels,\n",
    "                    batch_size=1000,\n",
    "                    epochs=10,\n",
    "                    verbose=1,\n",
    "                    validation_data=(test_images, test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = model.evaluate(test_images, test_labels, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
